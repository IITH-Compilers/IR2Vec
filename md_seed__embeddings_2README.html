<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>IR2Vec: Pretrained Embeddings</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">IR2Vec
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div id="doc-content">
<div><div class="header">
  <div class="headertitle"><div class="title">Pretrained Embeddings</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md59"></a></p>
<p>If you do not want to train the model to generate seed embeddings, and want to use the pretrained vocabulary, please use the vocabulary given <a href="../vocabulary">here</a> and skip the following.</p>
<p>Currently we support three different embedding dimensions (<code>75</code>, <code>100</code>, <code>300</code>). If you want to use embeddings of different dimensions, you can follow the below steps and copy the resultant vocabulary to the <a href="../vocabulary">vocabulary</a> directory with the following naming convention. <code>seedEmbeddingVocab&lt;DIM&gt;D.txt</code>. Such vocabularies would be automatically used during the build process of IR2Vec.</p>
<h1><a class="anchor" id="autotoc_md60"></a>
Generation of Seed Embedding Vocabulary</h1>
<p>This directory helps in generating seed embedding vocabulary in 3 steps.</p><ol type="1">
<li>Building ir2vec</li>
<li>Generating Triplets</li>
<li>Training TransE to generate seed embedding vocabulary</li>
</ol>
<h2><a class="anchor" id="autotoc_md61"></a>
Step 1: Building <code>ir2vec</code></h2>
<p>If you have not done <code>make</code>, follow the following steps to build <code>ir2vec</code> binary.</p><ul>
<li>Go to the <code>build</code> directory (<code>cd ../build</code>)</li>
<li><code>make</code></li>
</ul>
<h2><a class="anchor" id="autotoc_md62"></a>
Step 2: Generating Triplets</h2>
<h4><a class="anchor" id="autotoc_md63"></a>
Steps to collect the triplets</h4>
<p>Run <code>triplets.sh</code> script with the required parameters Usage: <code>bash triplets.sh &lt;build dir&gt; &lt;No of opt&gt; &lt;llFile list&gt; &lt;output FileName&gt;</code></p><ul>
<li><code>buildDir</code> points to the path of IR2Vec's build folder</li>
<li><code>numOpt</code> is an integer between <code>1</code> and <code>6</code><ul>
<li>Determines number of optimization sequences to apply on each file.</li>
<li>Optimization sequence can be one of the standard sequences <code>O[0-3sz]</code> selected at random</li>
</ul>
</li>
<li><code>llFileList</code> is a file containing the path of all the ll files. Use <code>find &lt;ll_dir&gt; -type f &gt; files_path.txt</code></li>
<li><code>outputFileName</code> is the file where the triplets would be written<ul>
<li>New file would be created if the file with given name <em>does not</em> exist.</li>
<li>If file <em>exists</em> with the given name, the triplets would be <b>appended</b> on to the same file.</li>
</ul>
</li>
</ul>
<p>Example Usage: </p><blockquote class="doxtable">
<p>&zwj;bash triplets.sh ../build 2 files_path.txt triplets.txt </p>
</blockquote>
<h4><a class="anchor" id="autotoc_md64"></a>
Files used to generate Seed Embedding Vocabulary</h4>
<p>We generated ll files from <code>Boost</code> libraries and <code>spec cpu 2017</code> benchmarks to generate triplets.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Dataset   </th><th class="markdownTableHeadNone">Source    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Boost   </td><td class="markdownTableBodyNone"><a href="https://www.boost.org/">https://www.boost.org/</a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">SPEC17 CPU   </td><td class="markdownTableBodyNone"><a href="https://www.spec.org/cpu2017/">https://www.spec.org/cpu2017/</a>   </td></tr>
</table>
<h2><a class="anchor" id="autotoc_md65"></a>
Step 3: Training TransE to generate seed embedding vocabulary</h2>
<p>The <a href="./OpenKE"><code>OpenKE</code></a> directory is a modified version of OpenKE repository (<a href="https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch">https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch</a>) with the necessary changes for training seed embedding vocabulary.</p>
<p>Please see <a class="el" href="md_seed__embeddings_2OpenKE_2README.html">OpenKE/README.md</a> for further information on OpenKE.</p>
<h4><a class="anchor" id="autotoc_md66"></a>
Requirements</h4>
<p>Create <code>conda</code> environment and install the packages given in <a href="./OpenKE/openKE.yaml">openKE.yaml</a></p><ul>
<li><code>conda create -f ./OpenKE/openKE.yaml</code></li>
<li><code>conda activate openKE</code></li>
</ul>
<h4><a class="anchor" id="autotoc_md67"></a>
Preprocessing the triplets</h4>
<p>We preprocess the generated triplets from the previous step in a form suitable for training TransE.</p><ul>
<li><code>cd OpenKE</code></li>
<li><code>python preprocess.py --tripletFile=&lt;tripletsFilePath&gt;</code><ul>
<li><code>--tripletFile</code> points to the location of the <code>outputFileName</code> generated in the previous step</li>
<li>The processed files <code>entity2id.txt</code>, <code>train2id.txt</code> and <code>relation2id.txt</code> will be generated in the same directory as that of <code>tripletsFilePath</code>. </li>
</ul>
</li>
</ul>
<h4><a class="anchor" id="autotoc_md68"></a>
Training TransE to generate embeddings</h4>
<p>Run <code>python generate_embedding_ray.py</code> <b>Possible Arguments:</b> All the arguments have default values unless provided:</p><ul>
<li><code>--index_dir</code>: Specifies the directory containing the processed files generated from preprocessing the triplets.</li>
<li><code>--epoch</code>: Sets the number of epochs. Default is <code>1000</code>.</li>
<li><code>--is_analogy</code>: Boolean flag to report analogy scores, calculated every 10 epochs using analogies.txt. Default is <code>False</code>.</li>
<li><code>--link_pred</code>: Boolean flag to report link prediction scores. Requires testing files (<code>test2id.txt</code>,<code>valid2id.txt</code>) in the `<code>--index_dir</code>. Link prediction scores include hit@1, hit@3, hit@10, mean rank (MR), and mean reciprocal rank (MRR). Default is <code>False</code>.</li>
<li><code>--nbatches</code>: Specifies the batch size. Default is <code>100</code>.</li>
<li><code>--margin</code>: Specifies the margin size for training. Default is <code>1.0</code>. </li>
</ul>
<h5><a class="anchor" id="autotoc_md69"></a>
Example Command</h5>
<p>To train a model with analogy scoring enabled and a batch size of 200, you can run: </p><div class="fragment"><div class="line">python generate_embedding_ray.py --index_dir &quot;../seed_embeddings/preprocessed/&quot; --epoch 1500 --is_analogy True --use_gpu true</div>
</div><!-- fragment --> <h5><a class="anchor" id="autotoc_md70"></a>
TensorBoard Tracking</h5>
<p>Once training begins, you can monitor the progress using TensorBoard by running the following command: </p><div class="fragment"><div class="line">tensorboard --logdir=~/ray_results</div>
</div><!-- fragment --> <h5><a class="anchor" id="autotoc_md71"></a>
ASHA Scheduler for Hyperparameter Optimization</h5>
<p>We employ the <a href="https://docs.ray.io/en/latest/tune/api/doc/ray.tune.schedulers.AsyncHyperBandScheduler.html#ray.tune.schedulers.AsyncHyperBandScheduler">ASHA Scheduler</a> to efficiently optimize hyperparameters and terminate suboptimal trials. This scheduler tracks key metrics, which are determined by the following conditions:</p>
<ul>
<li>If <code>--is_analogy</code> is set to <code>True</code>, the AnalogyScore will be the key metric.</li>
<li>If <code>--link_pred</code> is set to <code>True</code>, the hit@1 will be the key metric.</li>
<li>If neither flag is set, the default loss will be used as the key metric. </li>
</ul>
<h4><a class="anchor" id="autotoc_md72"></a>
Results</h4>
<p>Once the training completes, the best model will be saved in the specified <code>index_dir</code> with the filename format: </p><div class="fragment"><div class="line">seedEmbedding_{}E_{}D_{}batches_{}margin.ckpt</div>
</div><!-- fragment --><p> In addition, the entity embeddings will be stored in the <code>index_dir/embeddings</code> subdirectory in the following format: </p><div class="fragment"><div class="line">embeddings/seedEmbedding_{}E_{}D_{}batches_{}margin.txt</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
